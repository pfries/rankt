#!/usr/bin/env python
# -- coding: utf-8 --
from __future__ import print_function
from collections import namedtuple
from datetime import datetime
from snapshot import Snapshot, diff_last_two
from snapshot import diff_last_two
from store import store
from parser import parse
from top import Top
from hashlib import sha1
import os, json, time, sys, yaml, difflib, copy

def _load_master_config():
    master_config = os.path.join(
            os.path.expanduser('~'),
            '.rankt/config.yml')
    with open(master_config) as mc:
        return yaml.load(mc)

def _cd_project_root():
    while not os.path.exists('.rankt'):
        os.chdir('..')
        if os.getcwd() == os.path.expanduser('~'):
            raise RuntimeError('Command must be run from rankt project')

def _merge(properties, overrides):
    if isinstance(properties, dict) and isinstance(overrides, dict):
        for k,v in overrides.items():
            if k not in properties:
                properties[k] = v
            elif not isinstance(v,dict):
                # simple values override
                properties[k] = v
            else:
                # merge nested dicts
                properties[k] = _merge(properties[k],v)
    return properties

def _load_configs(args):
    '''
    Load the config files for the current context.
    '''
    config = _merge({}, _load_master_config())

    # find the project config
    cwd = os.getcwd()
    _cd_project_root()
    # load the project config
    if os.path.exists('config.yml'):
        with open('config.yml') as c:
            config = _merge(config, yaml.load(c))
    # back to start and check for local config
    os.chdir(cwd)

    # local config
    if os.path.exists('config.yml'):
        with open('config.yml') as c:
            config = _merge(config, yaml.load(c))

    # args overrides
    config = _merge(config, vars(args)) 

    return config

def init(args):
    '''
    Initialize a rankt project.
    '''
    if os.path.exists('.rankt'):
        raise RuntimeError('Directory already initialized.')

    config = _load_master_config()

    try:
        os.makedirs(config['cases_path'])
        os.makedirs(config['queries_path'])
    except OSError:
        raise RuntimeError('Found cases. Aborting.')

    project_config = {
            'project_name': args.project,
            'search_url': args.search_url
            }
    
    with open('config.yml', 'w') as c:
        c.write(yaml.dump(project_config, default_flow_style=False))

    open('.rankt', 'a').close() # touch .rankt file
    
def _parse_queries(query_arg):
    args = query_arg.split('=')
    query_name = args[0]
    query = {
            query_name: { 'args':None }
            }
    if(len(args) > 1):
        # has query template args
        query[query_name]['args'] = json.loads(args[1])
    return query

def _parse_query_option(query_arg):
    args = query_arg.split('=')
    query_name = args[0]
    query_args = None
    # only parse override properties
    if(len(args) > 1):
        query_args = json.loads(args[1])
    return query_name, query_args

def create(args):
    '''
    Create a rankt case.
    '''
    if not os.path.exists('.rankt'):
        raise Exception('Must run command from project root.')

    config = _load_configs(args)
    try:
        cwd = os.getcwd()
        os.chdir(config['cases_path'])
        os.makedirs(config['case'])
        os.chdir(config['case'])
        os.makedirs(config['queries_path'])

        default_kw = 'keywords'

        if args.keywords:
            with open(args.keywords) as f:
                with open(default_kw, 'w') as q:
                    for line in f:
                        q.write(line)
        elif not sys.stdin.isatty():
            with open(default_kw, 'w') as q:
                for line in sys.stdin:
                    q.write(line)

    except OSError as ose:
        raise RuntimeError('Cannot create case.') from ose

def _top(config):
    query_name, query_args = _parse_query_option(config['query'])
    if query_args:
        config = _merge(config, { 
            'queries': { 
                query_name: { 
                    'args': query_args
                    }
                }
            })
    return Top(config).top(query_name, config['keywords'])

def _read_keywords(args):
    keywords = []
    if args.keywords:
        with open(args.keywords) as f:
            for kw in f:
                keywords.append(kw)
    elif not sys.stdin.isatty():
        for kw in sys.stdin:
            keywords.append(kw)
    
    return keywords

def top(args):
    config = _load_configs(args)
    config['keywords'] = _read_keywords(args)
    top_hits = _top(config)
    print(json.dumps(top_hits))

def diff(args):
    config = _load_configs(args)
    config['keywords'] = _read_keywords(args)
    top_hits1,top_hits2 = [],[]
    if args.with_query:
        # treat hits as queries to run
        config['query'] = args.hits[0]
        top_hits1 = _top(config)
        config['query'] = args.hits[1]
        top_hits2 = _top(config)
    else:
        with open(args.hits[0]) as h1, open(args.hits[1]) as h2:
            top_hits1 = json.loads(h1.read())
            top_hits2 = json.loads(h2.read())

    differ = difflib.SequenceMatcher()
    kw_churn = [] 
    for idx, kw in enumerate(top_hits1):
        seq1 = []
        current_kw = ''
        for doc in kw:
            seq1.append(doc['documentid'])
            current_kw = doc['keywords']
        seq1 = tuple(seq1)

        kw2 = top_hits2[idx]
        seq2 = []
        for doc in kw2:
            seq2.append(doc['documentid'])
        seq2 = tuple(seq2)

        differ.set_seqs(seq1,seq2)
        unordered_similarity = differ.ratio()
        differ.set_seqs(sorted(seq1),sorted(seq2))
        sorted_similarity = differ.ratio()
        insertions = len(set(seq1) - set(seq2))
        churn_score = unordered_similarity * (1/insertions if insertions > 0 else 1)

        kw_churn.append({
                "keywords": current_kw,
                "similarity": unordered_similarity,
                "sorted_similarity": sorted_similarity,
                "insertions": insertions,
                "churn_score": churn_score
                })

    print(json.dumps(kw_churn))


def snapshot(args):
    args.run_datetime = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')
    args.run_timestamp = str(int(time.time()))

    config = make_config(args)
    config['query'] = _parse_queries(config['query'])

    ss = Snapshot(config).snapshot()
    print(json.dumps(ss))

def diff2(args):
    queries = copy.deepcopy(config['query'])
    if len(queries) != 2:
        raise ValueError('Diff requires two queries')

    top_hits = []
    for q in queries:
        config['query'] = [q]
        ss = Top(config).top()
        snapshots.append(ss)

    differ = difflib.SequenceMatcher()
    snapshot1,snapshot2 = tuple(snapshots)
    kw_churn = [] 
    for idx, kw in enumerate(snapshot1):
        seq1 = []
        current_kw = ''
        for doc in kw:
            seq1.append(doc['documentid'])
            current_kw = doc['keywords']
        seq1 = tuple(seq1)

        kw2 = snapshot2[idx]
        seq2 = []
        for doc in kw2:
            seq2.append(doc['documentid'])
        seq2 = tuple(seq2)

        differ.set_seqs(seq1,seq2)
        unordered_similarity = differ.ratio()
        differ.set_seqs(sorted(seq1),sorted(seq2))
        sorted_similarity = differ.ratio()
        insertions = len(set(seq1) - set(seq2))
        churn_score = unordered_similarity * (1/insertions if insertions > 0 else 1)

        kw_churn.append({
                "keywords": current_kw,
                "similarity": unordered_similarity,
                "sorted_similarity": sorted_similarity,
                "insertions": insertions,
                "churn_score": churn_score
                })

    print(json.dumps(kw_churn))

def make_config(args):
    with open(args.master_config) as m, open('config.yml') as p:
        config = yaml.load(m)
        project_config = yaml.load(p)
        config.update(project_config)
        config.update(vars(args))
        keywords = []
        if args.keywords:
            with open(args.keywords) as f:
                for kw in f:
                    keywords.append(kw)
        elif not sys.stdin.isatty():
            for kw in sys.stdin:
                keywords.append(kw)
        
        config['keywords'] = keywords
        if args.__dict__.get('case'):
            config['case'] = args.case
            config['case_path'] = os.path.join(config['cases_path'], args.case)
            config['queries_path'] = os.path.join(config['case_path'], config['queries_path'])
        return config

def format_output(response, format):
    if format == 'json':
        return json.dumps(parse(response))
    elif format == 'text':
        return str(parse(response))
    else:
        print(response)


def store_document(args):
    config = make_config(args)

    for j in sys.stdin:
        docs = json.loads(j)
        for doc in docs:
            joined_id = '{}{}{}{}{}'.format(
                    doc['project'],
                    doc['case'],
                    doc['keywords'],
                    doc['snapshot'],
                    doc['documentid'])
        
            hashed_id = sha1(joined_id.encode()).hexdigest()
            endpoint = '/'.join([config['store_endpoint'], args.store_type, hashed_id])
            store(endpoint, doc)

def main():
    import argparse

    master_config = os.path.join(
            os.path.expanduser('~'),
            '.rankt/config.yml')

    parser = argparse.ArgumentParser()
    parser.add_argument('--config', '-c', dest='master_config', default=master_config)
    subparsers = parser.add_subparsers()

    init_parser = subparsers.add_parser('init', help='initialize a project')
    init_parser.add_argument('--search-url', '-u', required=True)
    init_parser.add_argument('project', nargs='?',
            default=os.path.basename(os.getcwd()), 
            help='name of the project (default: pwd)'
            )
    init_parser.set_defaults(func=init)

    create_parser = subparsers.add_parser('create', help='create a new case')
    create_parser.add_argument('case', help='the name of the case')
    create_parser.add_argument('--keywords', '-k', nargs='?')
    create_parser.set_defaults(func=create)

    snapshot_parser = subparsers.add_parser('snapshot', help='snapshot a case')
    snapshot_parser.add_argument('case', help='the case to snapshot')
    snapshot_parser.add_argument('query', nargs='+', help='the queries to snapshot')
    snapshot_parser.add_argument('--size', '-s', help='number of docs to fetch')
    snapshot_parser.add_argument('--keywords', '-k', nargs='?')
    snapshot_parser.add_argument('-t', '--tags', nargs='*', help='tag for snapshot')
    snapshot_parser.set_defaults(func=snapshot)

    top_parser = subparsers.add_parser('top', help='get top results')
    top_parser.add_argument('query', help='the query')
    top_parser.add_argument('--size', '-s', help='number of results')
    top_parser.add_argument('--keywords', '-k', nargs='?')
    top_parser.add_argument('--case', '-c',
            default=os.path.basename(os.getcwd()), 
            help='the case (default: pwd)'
            )
    top_parser.set_defaults(func=top)

    diff_parser = subparsers.add_parser('diff', help='diff search results')
    diff_parser.add_argument('hits', nargs=2, help='the hits to diff')
    diff_parser.add_argument('--with-query', '-q', action='store_true', help='diff queries')
    diff_parser.add_argument('--size', '-s', help='number of docs to fetch')
    diff_parser.add_argument('--keywords', '-k', nargs='?')
    diff_parser.add_argument('--case', '-c',
            default=os.path.basename(os.getcwd()), 
            help='the case (default: pwd)'
            )
    diff_parser.set_defaults(func=diff)

    store_parser = subparsers.add_parser('store', help='store documents')
    store_parser.add_argument('store_type', help='the type of the document')
    store_parser.set_defaults(func=store_document)

    args = parser.parse_args()
    args.func(args)

if __name__ == '__main__':
    main()
